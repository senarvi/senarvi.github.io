---
title: "ICLR 2020"
permalink: /notes/iclr2020/
author_profile: true
math: true
---

## [Variational Template Machine for Data-to-Text Generation](http://www.openreview.net/pdf?id=HkejNgBtPB)

* A graphical model for generating text <span>$y$</span> from structured data <span>$x$</span>
* Similar to a variational autoencoder, but adds latent variables that represent a template <span>$z$</span> and content <span>$c$</span>
* Continuous latent variables generate diverse output
* Reconstruction loss for output given data and template
* Template preserving loss: template variable can reconstruct the text

<figure>
  <img src="/assets/images/variational-template-machine.png" style="border-width: 15px 45px 15px 45px">
  <figcaption>
    The graphical model of a Variational Template Machine.
    <a href="http://www.openreview.net/pdf?id=HkejNgBtPB">Ye et al.</a>
  </figcaption>
</figure>

### [Variational Inference](https://fabiandablander.com/r/Variational-Inference.html)

* Approximation for Bayesian inference with graphical models
* Computing posterior probabilities is hard because of the integration required to compute the evidence probability
* Approximates the posterior distribution using a variational posterior from a family of distributions
* Computing the KL divergence between the approximate and exact posterior would be equally hard, since it depends on the evidence probability
* Minimizing KL divergence is equal to minimizing the negative evidence lower bound (ELBO)
* ELBO is log evidence subtracted from the KL divergence
* The optimization problem is not as hard as integration

## [Meta-Learning with Warped Gradient Descent](http://www.openreview.net/pdf?id=rkeiQlBFPB)

* Model consists of task layers and warp layers
* Task layers are updated using a normal loss
* Meta-learn warp-layers that modify the loss surface to be smoother
* Novelty: non-linear warp layers avoid dependence on the trajectory or the initialization, depending only on the task parameter updates at the current position of the search space

### Meta-Learning

* Learning some parameters of the optimizer such as initialization
* Optimize for the final model accuracy as a function of the initialization
* When using gradient descent, the objective is differentiable
* Backpropagate through all training steps back into the initialization
* Vanishing / exploding gradients when the loss surface is very flat or steep
* Costly—usually scales to a handful of training steps only

## [Transformer-XH](http://www.openreview.net/pdf?id=r1eIiCNYwS)

* Adaptation of Transformer for structured text
* For example, multi-evidence reasoning: answer questions, following words that are linked to Wikipedia
* Extra hop attention attends to the first token of another sequence (representative of the entire sequence)

## [Deep Double Descent](http://www.openreview.net/pdf?id=B1g5sA4twr)

* Bigger model means lower training loss
* At some point test error starts to increase, but with large enough models decreases again
* Occurs across various architectures (CNNs, ResNets, Transformers), data domains (NLP, vision), and optimizer (SGD, Adam)
* Also occurs when increasing training time
* In some cases increasing training data can hurt performance
* Effective model complexity takes training time into account

## [Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization](http://www.openreview.net/pdf?id=SkgGjRVKDS)

* Unstable mean and variance estimation with too small batch sizes
* Batch renormalization (BRN): corrects batch mean and variance by moving average
* Moving average batch normalization: moving average of variance mean reduction + weight centralization

## [Compressive Transformer for Long-Range Sequence Modelling](https://openreview.net/pdf?id=SylKikSYDH)

* Train a language model on segments similar to Transformer-XL
* When moving to the next segment, the oldest N activations in the memory are compressed using a compressing function

### Related Work

* Lightweight and Dynamic Convolution: depth-wise separable convolution that runs in linear time
* Transformer-XL: train a language model on segments, but include activations from the previous segment in “extended context”
* Sparse Transformer: sparse attention masks
* Adaptive Attention Span: different attention heads can have longer or shorter spans of attention

## [A Closer Look at Deep Policy Gradients](https://openreview.net/pdf?id=ryxdEkHtPS)

* Policy gradient methods: optimize policy parameters to maximize the expected reward
* Variance reduction using baseline—separate the quality of the action from the quality of the state
* A canonical choice of baseline function is the value function
* Surrogate objective is a simplification of reward maximization used by modern policy gradient algorithms (policy is divided by the old policy, [Schulman et al. 2015](https://arxiv.org/abs/1502.05477))
* Measure of gradient variance: mean pairwise correlation (similarity) between gradient samples
* Visualization of optimization landscapes with different number of samples per estimate

## [Laurent Dinh: Invertible Models and Normalizing Flows](https://iclr.cc/virtual_2020/speaker_4.html)

* A model finds a representation <span>$h = f(x)$</span> of a datapoint <span>$x$</span> (in practice, an image)
* Generative models (VAE, GAN) can produce an image <span>$x$</span> from its representation <span>$h$</span>
* *Normalizing flow* is a sequence of invertible transformations
* Reversible generative models can encode an image into a latent space, making it possible to interpolate between two images

### [Variational Autoencoder (VAE)](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)

* A standard autoencoder learns representations with distinct clusters in the latent space
* VAE encoder produces a set of means and variances, and then samples the inputs of the decoder
* The latent space is continuous
* Only approximate inference of latent variables from a datapoint

<figure>
  <img src="/assets/images/mnist-autoencoder.png">
  <figcaption>
    Training an autoencoder on MNIST results in distinct clusters.
    <a href="http://dx.doi.org/10.1126/science.1127647">Hinton and Salakhutdinov</a>.
  </figcaption>
</figure>

### Generative Adversarial Network (GAN)
* A generator network creates fake images and a discriminator network learns to distinguish them from real images
* Images cannot be encoded into the latent space

### [NICE](https://arxiv.org/pdf/1410.8516.pdf)

* Finds a representation such that <span>$p(h)$</span> factorizes as <span>$\prod p(h_d)$</span>, where <span>$h_d$</span> are independent latent variables (arguably a “good” representation)
* Prior distribution <span>$p(h_d)$</span> is Gaussian or logistic
* Training: maximize the likelihood of the data using a “change of variables” formula
* <span>$f(x)$</span> has to be invertible—achieved by splitting <span>$x$</span> into <span>$x_1$</span> and <span>$x_2$</span> and using a transformation that transforms them into <span>$x_1$</span> and <span>$x_2 + m(x_1)$</span> respectively
* Sampling images: sample from <span>$p(h)$</span> and use the inverse of <span>$f(x)$</span>

## [A Probabilistic Formulation of Unsupervised Text Style Transfer](http://www.openreview.net/pdf?id=HJlA0C4tPS)

* Change text style, keeping the semantic meaning unchanged
* Machine translation, sentiment transfer (positive ↔ negative)
* Unsupervised (only a nonparallel corpus)
* Previous work on text style transfer: autoencoding loss + adversarial loss using a language model as a discriminator
* Previous work on machine translation: cycle structures for unsupervised machine translation
* Novelty: probabilistic formulation of the above heuristic training strategies
* Translations of language A are “latent sequences” of language B and vice versa
* We want to utilize a language model prior within each language
* Train using variational inference

<figure>
  <img src="/assets/images/unsupervised-text-style-transfer.png">
  <figcaption>
    A graphical model for text style transfer.
    <a href="http://www.openreview.net/pdf?id=HJlA0C4tPS">He et al.</a>
  </figcaption>
</figure>

## [Estimating Gradients for Discrete Random Variables by Sampling without Replacement](http://www.openreview.net/pdf?id=rklEj2EFvB)

* Strategies for obtaining gradients for discrete outputs: smoothening the outputs (relaxation), or sampling (REINFORCE)
* REINFORCE: move the gradient inside the expectation and estimate it using a sample
* With multiple samples one can use the average as a baseline
* Sampling without replacement can be done efficiently by taking the top <span>$k$</span> of Gumbel variables
* Probability for sampling an unordered sample can be calculated as a sum over all possible permutations
* The estimator is changed to work with an unordered set
