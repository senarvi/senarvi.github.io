---
title: "Generative Models"
permalink: /notes/generative-models/
author_profile: true
math: true
---

* Typical scenario: a model finds a representation <span>$h = f(x)$</span> of a datapoint (image) <span>$x$</span>
* Generative models can produce an image <span>$x$</span> from its representation <span>$h$</span>

## Autoencoders

* An encoder network followed by a decoder network
* Encoder compresses the data into a lower-dimensional vector
* Given powerful enough decoder, in theory the original datapoint could be perfectly reconstructed even from a one-dimensional latent representation
* A standard autoencoder learns representations with distinct clusters and lack of regularity in the latent space
* In order to generate new content we would need a way to sample meaningful latent representations

### [Variational Autoencoder (VAE)](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)

* VAE encoder produces two vectors: means and variances for a set of random variables
* The input of the decoder is a sample of these random variables
* The latent space is continuous

<figure>
  <img src="/assets/images/mnist-autoencoder.png">
  <figcaption>
    Training an autoencoder on MNIST results in distinct clusters.
    <a href="http://dx.doi.org/10.1126/science.1127647">Hinton and Salakhutdinov</a>.
  </figcaption>
</figure>

## [Generative Adversarial Nets (GAN)](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)

* Simultaneously train two models, generator <span>$G$</span> and discriminator <span>$D$</span>
* <span>$G$</span> learns a mapping from a prior noise distribution to the data space
* <span>$D$</span> predicts the probability that a sample is from the training data rather than was generated by <span>$G$</span>
* A conditional GAN is obtained by adding an additional input to both <span>$G$</span> and <span>$D$</span> (for example, image category or an input image)

### [pix2pix](https://arxiv.org/abs/1611.07004)

* The difficulty with training an image-to-image network is what loss to optimize
* For example, Euclidean distance is minimized by averaging all plausible outputs, producing blurry results
* GAN learns the loss function automatically
* A conditional GAN that is conditioned on the input image is suitable for image-to-image translation
* Generator is based on the U-Net architecture
* PatchGAN discriminator penalizes structure at the scale of local image patches only
* The discriminator is run convolutionally across the image, averaging responses from all patches
* Additional L1 loss encourages the output to be similar to the ground truth

### [CycleGAN](https://junyanz.github.io/CycleGAN/)

* Two generators, <span>$G$</span> and <span>$F$</span>, translate images between two domains

<div>$$
\begin{align}
&G: X \to Y \\
&F: Y \to X
\end{align}
$$</div>

* Generator is a CNN consisting of an encoder, transformer, and a decoder
* Two discriminators, <span>$D_Y$</span> and <span>$D_X$</span>, try to distinguish real images from generated images
* Discriminator is a CNN that follows the PatchGAN architecture
* Adversarial loss for <span>$G$</span> makes <span>$D_Y$</span> distinguish <span>$G(x)$</span> from <span>$y$</span>:

<div>$$
\begin{align}
&D_Y(y) \to 0 \\
&D_Y(G(x)) \to 1
\end{align}
$$</div>

* Adversarial loss for <span>$F$</span> makes <span>$D_X$</span> distinguish <span>$F(y)$</span> from <span>$x$</span>:

<div>$$
\begin{align}
&D_X(x) \to 0 \\
&D_X(F(y)) \to 1
\end{align}
$$</div>

* Cycle consistency loss expresses that an image translation cycle should bring back the original image:

<div>$$
\begin{align}
&F(G(x)) \to x \\
&G(F(y)) \to y
\end{align}
$$</div>

## [Reversible Generative Models](https://iclr.cc/virtual_2020/speaker_4.html)

* GANs cannot encode images into the latent space
* VAEs support only approximate inference of latent variables from an image
* *Normalizing flow* is a sequence of invertible transformations
* Reversible generative models can encode an image into a latent space, making it possible to interpolate between two images
